{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_8Ln3xYMMX_"
      },
      "source": [
        "# GPT로 영화 리뷰 생성 모델 학습하기\n",
        "\n",
        "이번 실습에서는 GPT를 imdb에서 제공하는 영화리뷰들로 학습하여 영화 리뷰를 만들어낼 수 있는 모델을 구현합니다.\n",
        "먼저 필요한 library들을 import합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im4AdHKyKArX",
        "outputId": "90735593-7b68-43ad-8e9e-30d25ac02afe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1725441826118,
          "user_tz": -540,
          "elapsed": 19490,
          "user": {
            "displayName": "박응수",
            "userId": "03296589907204334058"
          }
        }
      },
      "source": [
        "!pip install datasets"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5UW3eXFMk09"
      },
      "source": [
        "이후 저번 주차와 같은 코드로 tokenizer를 준비합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHyxgdeXKArZ",
        "outputId": "bade5670-9b6d-4099-90b7-bf566fd32abd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371,
          "referenced_widgets": [
            "4a25b35f448644a3b786fc547439f82d",
            "6b48ade5c68f4fc8896c807011a9c255",
            "791a376048074b1db2107065262d4c5b",
            "67723ca2c63f49029111f7856222fa2d",
            "6047d40d77284f96bd1e72ccebfe48db",
            "54cfdb5730164683a3913c3f09b57a80",
            "0ec3d146dbd546459a856079f7b2ef32",
            "baa1d5d4d8994ef1a354121d042a2bde",
            "4373f3712e9242ab87f2ce628e1c3825",
            "4923fdd3d2c0483780d3996cde10a4e4",
            "78f35700711e4107a296166dcfaacafe",
            "1ccb48067995412dbc002afadb7a8ec5",
            "8328b1bcdaab42dbad9b1c74995f6cc9",
            "39d417c2f2fc49d0b38dcbdff60b5cba",
            "27b4f303f0674a4897f206ee17361e91",
            "71920ed6667a42629585666c8f6a42c5",
            "6119932bd56045b8a06c6ee3aba665d2",
            "63343a5f39bb4b71ba92bdbd9d339b22",
            "13217fe16c364a7385b8b21556b27de4",
            "087ba3b1f366403d972d2d8196aa79d1",
            "a39c3814f3b54a04977d04698173d112",
            "9a96747a090149ce94481d46aef9f74f",
            "510b9be53b8348779fdc18df2993fa1d",
            "e974a9ef986d4bbbb5328fefd07b82ce",
            "da086b03f6de43399577d14d48761d62",
            "004b719d1db44fb190aad0a65935f829",
            "a2119c3a57214f8a82d3f0c2bdb7174a",
            "5ea6c43df2b945258f5ba4fd6aba730c",
            "075184d40ec64074991a3e9cebdf1676",
            "1f5f5e607d244797aee529aeedb1122f",
            "250fdc54fe3a452888e9d4d49e1e2cb0",
            "fa8d1b100a9b453db6b6c62f0c4dc95d",
            "5ffb909d025b4ee3901c0fccde99d499",
            "ab989235df864150beda2ce68b2ee7dd",
            "441a9a93ac824d74b23473bfe0f43977",
            "8fcb44511f2644cf89a989289b064dec",
            "3f178a3a5b004e0ea94ec5c80fd3bfca",
            "4d016f0e2d124750b82ad23b7067e85f",
            "16f580e4e9f242318918909d4f61688a",
            "ce013c5c658a4b4c9a23118fcff2497d",
            "784da324fa4f4005b3714c9e4d9b8ccb",
            "5db8cb4fc99e4111af17734e85147de6",
            "55a08daca01843dd96cd3587a21f3762",
            "91261cde1bf645188f1928871caff6e3",
            "51c9e94c3f484cf2990bbd66c1fb389c",
            "81afa7dd498f46f790b0066ff76f3bb7",
            "f9777975171442ccb236dcea9afe1566",
            "6556e92186084349849c187866fcd9ec",
            "275e5f0cfb664421bbc30590b2bd1ede",
            "328e3d8ac45e4cb58f5587bcf56ab8f4",
            "a2af872b81494e2e8b8145f80436bb31",
            "f8eda6dd84284cc69bad076ec20019b8",
            "ea9dadf3bf154cc583ea252ced8afcbf",
            "3cdce6d0d4c7434e8e56cf1e13a924cb",
            "814f82bf44cb442ca54b1d0fd857844c",
            "383d22d8a98a4bd4958c7ce796d4d5ce",
            "5257eae968434a4d93bb541de5f55abd",
            "619f922fab854fc296f201afa09d422b",
            "c79f1188a7264ba5a79a555318c688d4",
            "e1d7e90359a0444098bbcd39e3b11c88",
            "5b02cf1d9e0940cf80f52387d2604dbb",
            "21a10c64d3b64b319949b2f16356e155",
            "088911a296bf460a8790da6839f71f9a",
            "eace43b3402341089989313ace8c0d4a",
            "a44a18af706c4af4a62210de63823b57",
            "130036817a4a48988c8aa5549d6034dc",
            "d14952c478aa499783ddc851d9a1a590",
            "05d3e70ff3f24336b2aba6fe14275c82",
            "ee6b5b3d415a4ff98c98eeb6d721c9e1",
            "5509598ce2764dcb98b0e08d17490e0a",
            "0e448049bc334072ba45e3fafd3e7adb",
            "1b4522b7577d4f33854bb44602901d8b",
            "f170938e5be944d7b8106449262e11db",
            "4deb09c5dd24415fa127fe0bf75040f3",
            "93d65fb4fea74b7ba496913180e9219c",
            "f537da112c4e49c48d8a7300d6433f37",
            "310e1f5e682c4eb989c65a5a18bb3bc7"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1725441865828,
          "user_tz": -540,
          "elapsed": 39714,
          "user": {
            "displayName": "박응수",
            "userId": "03296589907204334058"
          }
        },
        "ExecuteTime": {
          "end_time": "2025-04-15T19:17:02.820007Z",
          "start_time": "2025-04-15T19:16:53.018288Z"
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "# Hugging Face datasets 라이브러리에서 IMDB 데이터셋을 로드\n",
        "ds = load_dataset(\"stanfordnlp/imdb\")\n",
        "\n",
        "# 새로운 WordPiece 토크나이저 초기화\n",
        "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "\n",
        "# BERT와 유사한 정규화 설정 (소문자 변환 및 기타 텍스트 조정)\n",
        "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
        "\n",
        "# BERT와 유사한 사전 토크나이징 설정 (텍스트를 단어로 분리)\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
        "\n",
        "\n",
        "def get_training_corpus():\n",
        "    # 데이터셋에서 텍스트 데이터의 청크를 생성하는 제너레이터 함수\n",
        "    for i in range(0, len(ds['train']), 1000):\n",
        "        yield ds['train'][i: i + 1000]['text']\n",
        "\n",
        "\n",
        "# 토크나이저를 위한 특수 토큰 정의\n",
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\"]\n",
        "\n",
        "# 지정된 어휘 크기와 특수 토큰으로 토크나이저를 훈련시키기 위한 WordPieceTrainer 초기화\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=10000, special_tokens=special_tokens)\n",
        "\n",
        "# 훈련된 토크나이저를 사용하여 텍스트 데이터로 훈련\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
        "\n",
        "# Hugging Face Transformers 라이브러리와 호환되는 형식으로 훈련된 토크나이저로 변환\n",
        "tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)\n"
      ],
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rlgus\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5onMJfrbMpRm"
      },
      "source": [
        "이번에는 next word prediction을 위한 data를 준비합니다. 다음과 같이 `collate_fn`을 수정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k0DV9aFKMBF",
        "ExecuteTime": {
          "end_time": "2025-04-15T19:17:39.752593Z",
          "start_time": "2025-04-15T19:17:39.749067Z"
        }
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # 최대 길이 설정\n",
        "    max_len = 400\n",
        "    texts, labels = [], []\n",
        "\n",
        "    for row in batch:\n",
        "        # 텍스트를 토크나이징하고, input_ids를 얻어와서 라벨로 사용\n",
        "        # [1:]은 [CLS] 토큰을 제거하기 위함\n",
        "        labels.append(torch.LongTensor(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[1:]))\n",
        "\n",
        "        # [:-1]은 [SEP] 토큰을 제거하기 위함\n",
        "        texts.append(torch.LongTensor(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[:-1]))\n",
        "\n",
        "    # 배치 내 모든 텍스트 시퀀스를 패딩하여 같은 길이로 맞춤\n",
        "    # padding_value는 [PAD] 토큰의 ID\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    # 배치 내 모든 라벨 시퀀스를 패딩하여 같은 길이로 맞춤\n",
        "    # padding_value는 [PAD] 토큰의 ID\n",
        "    labels = pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    return texts, labels\n"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bRnU960MuCd"
      },
      "source": [
        "이전 과제인 last word prediction과 달라진 점은 label은 `input_ids[1:]`를, text는 `input_ids[:-1]`를 사용한다는 것입니다.\n",
        "이렇게 바꾸어 모든 가능한 input text의 segment에 대해 다음 단어를 예측할 수 있도록 합니다.\n",
        "\n",
        "다음은 data loader와 모델을 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-80oHBTKO7l",
        "ExecuteTime": {
          "end_time": "2025-04-15T19:18:38.654550Z",
          "start_time": "2025-04-15T19:18:38.652091Z"
        }
      },
      "source": [
        "train_loader = DataLoader(\n",
        "    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DB0OvCB6KA7",
        "ExecuteTime": {
          "end_time": "2025-04-15T19:18:58.380832Z",
          "start_time": "2025-04-15T19:18:58.371832Z"
        }
      },
      "source": [
        "from torch import nn\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.wq = nn.Linear(input_dim, d_model)\n",
        "        self.wk = nn.Linear(input_dim, d_model)\n",
        "        self.wv = nn.Linear(input_dim, d_model)\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
        "        B, S, D = q.shape[0], q.shape[1], self.d_model // self.n_heads\n",
        "\n",
        "        q = q.reshape((B, S, self.n_heads, D)).transpose(1, 2)\n",
        "        k = k.reshape((B, S, self.n_heads, D)).transpose(1, 2)\n",
        "        v = v.reshape((B, S, self.n_heads, D)).transpose(1, 2)\n",
        "\n",
        "        score = torch.matmul(q, k.transpose(-1, -2))  # (B, H, S, D) * (B, H, D, S) = (B, H, S, S)\n",
        "        score = score / sqrt(self.d_model)\n",
        "\n",
        "        if mask is not None:\n",
        "            score = score + (mask[:, None] * -1e9)\n",
        "\n",
        "        score = self.softmax(score)\n",
        "        result = torch.matmul(score, v)  # (B, H, S, D)\n",
        "\n",
        "        result = result.transpose(1, 2).reshape((B, S, -1))\n",
        "        result = self.dense(result)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_heads, dff):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.dff = dff\n",
        "\n",
        "        self.sa = MultiHeadAttention(input_dim, d_model, n_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model)\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x1 = self.sa(x, mask)\n",
        "        x1 = self.dropout1(x1)\n",
        "        x1 = self.norm1(x + x1)\n",
        "\n",
        "        x2 = self.ffn(x1)\n",
        "        x2 = self.dropout2(x2)\n",
        "        x2 = self.norm2(x1 + x2)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "\n",
        "    return torch.FloatTensor(pos_encoding)"
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf-nMhvQM_v2"
      },
      "source": [
        "이번에는 Transformer를 가지고 GPT를 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MaiCGh8TsDH",
        "ExecuteTime": {
          "end_time": "2025-04-15T19:19:59.137809Z",
          "start_time": "2025-04-15T19:19:59.125574Z"
        }
      },
      "source": [
        "max_len = 400\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_layers, dff):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.dff = dff\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "        self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, n_heads, dff) for _ in range(n_layers)])\n",
        "        self.classification = nn.Linear(d_model, len(tokenizer))\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        mask1 = (x == tokenizer.pad_token_id)[..., None]  # (B, S, 1)\n",
        "        mask2 = torch.tril(torch.ones(seq_len, seq_len)).type(torch.ByteTensor).to(x.device)[None]  # (B, S, S)\n",
        "        mask = mask1 & mask2\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = x * sqrt(self.d_model)\n",
        "        x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        x = self.classification(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = GPT(len(tokenizer), 32, 4, 5, 32)"
      ],
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z9o6i3QN9sO"
      },
      "source": [
        "이전에 구현한 `TextClassifier`와 유사하지만 다음 두 가지 차이점이 있습니다.\n",
        "\n",
        "1. `mask`가 다르게 설정됩니다. 기존의 `mask`는 padding token만을 걸러냈다면, 이번에는 생성모델의 취지에 맞춰 예측하고자 하는 token의 representation을 결정할 때, 미래의 token들에는 영향을 받지 않도록 mask를 계산합니다. 이는 `mask2`에 해당합니다.\n",
        "2. Classifier를 첫 번째 token 가지고 하는 것이 아닌 모든 token에 대해서 진행합니다.\n",
        "\n",
        "이제 GPT를 학습해보고, 예측 결과를 이전과 같은 코드를 사용하여 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHVVsWBPQmnv",
        "outputId": "e499457f-0a96-4f94-e7a0-249d627a148e",
        "ExecuteTime": {
          "end_time": "2025-04-15T19:27:05.092725Z",
          "start_time": "2025-04-15T19:20:01.159308Z"
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "lr = 0.001\n",
        "model = model.to('cuda')\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    total_loss = 0.\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        model.zero_grad()\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        preds = model(inputs)\n",
        "\n",
        "        preds = preds.reshape((-1, len(tokenizer)))\n",
        "        labels = labels.reshape(-1)\n",
        "        mask = (inputs == tokenizer.pad_token_id)\n",
        "        mask = mask.reshape(-1)\n",
        "\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss = (loss * ~mask).sum() / (~mask).sum()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0 | Train Loss: 3140.047119617462\n",
            "Epoch   1 | Train Loss: 2304.50426197052\n",
            "Epoch   2 | Train Loss: 2180.224048614502\n",
            "Epoch   3 | Train Loss: 2125.603184223175\n",
            "Epoch   4 | Train Loss: 2091.9919662475586\n",
            "Epoch   5 | Train Loss: 2068.763642311096\n",
            "Epoch   6 | Train Loss: 2051.2913274765015\n",
            "Epoch   7 | Train Loss: 2037.3982892036438\n",
            "Epoch   8 | Train Loss: 2025.9655270576477\n",
            "Epoch   9 | Train Loss: 2016.210012435913\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAXB6GgIQy1S",
        "outputId": "d89e27f0-4eb3-4f52-8183-949f38e372e8",
        "ExecuteTime": {
          "end_time": "2025-04-15T19:28:21.786525Z",
          "start_time": "2025-04-15T19:28:21.768379Z"
        }
      },
      "source": [
        "input_text = \"I am \"\n",
        "tokens_org = tokenizer(input_text).input_ids\n",
        "tokens = torch.LongTensor(tokens_org)[None].to('cuda')\n",
        "\n",
        "last_token_pred = model(tokens)[0, -1].argmax()\n",
        "tokenizer.decode(tokens_org + [last_token_pred.item()])"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i am not'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8yudhipWE2T"
      },
      "source": [
        "보시다시피 loss도 잘 떨어지고 있고, \"i am \"이라는 문장을 넣었을 때 \"not\"이라는 token을 예측하여 말이 되는 생성 결과를 내놓는 것을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "del7uDKLWE2T"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}